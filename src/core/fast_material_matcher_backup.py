#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½æè´¨åŒ¹é…å™¨ - è¶…çº§ä¼˜åŒ–ç‰ˆæœ¬
"""

from typing import Dict, List, Optional
from src.core.material_matcher import MaterialMatcher
import time

class FastMaterialMatcher(MaterialMatcher):
    """é«˜æ€§èƒ½æè´¨åŒ¹é…å™¨"""
    
    def __init__(self, database_manager):
        super().__init__(database_manager)
        
        # æ€§èƒ½é…ç½®
        self.max_results_per_search = 10000  # å¤§å¹…æé«˜é™åˆ¶ï¼Œä¸»è¦ä¾é é˜ˆå€¼è¿‡æ»¤
        self.similarity_threshold_boost = 0.0  # ä¸æé«˜é˜ˆå€¼ï¼Œç¡®ä¿æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æè´¨
    
    def find_similar_materials_fast(self, source_material: Dict, target_library_id: int, 
                                  priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ä¸¤å±‚æœç´¢ç­–ç•¥ï¼š
        1. å¿«é€Ÿé¢„ç­›é€‰æœç´¢
        2. å¦‚æœç»“æœä¸º0ï¼Œè¿›è¡Œç²¾ç¡®æœç´¢
        """
        print(f"ğŸ¯ FastMaterialMatcher.find_similar_materials_fast() è¢«è°ƒç”¨ï¼ç±»å‹: {type(self)}")
        print(f"ğŸ¯ æ–¹æ³•è·¯å¾„: {self.__class__.__module__}.{self.__class__.__name__}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ­£ç¡®çš„å®ä¾‹
        if "MultiThreadMaterialMatcher" in str(type(self)):
            print("âš ï¸  æ³¨æ„ï¼šè¿™æ˜¯MultiThreadMaterialMatcherå®ä¾‹ï¼Œå¯èƒ½æœ‰æ–¹æ³•å†²çªï¼")
        try:
            print(f"\nğŸ” === å¼€å§‹ä¸¤å±‚æœç´¢ç­–ç•¥ === ğŸ”")
            print(f"ğŸ“ æºæè´¨: {source_material.get('filename', 'Unknown')}")
            print(f"ğŸ“Š ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}%")
            
            # ç¬¬ä¸€å±‚ï¼šå¿«é€Ÿé¢„ç­›é€‰æœç´¢
            print("ğŸš€ ç¬¬ä¸€å±‚ï¼šæ‰§è¡Œå¿«é€Ÿé¢„ç­›é€‰æœç´¢...")
            results = self._perform_fast_search(source_material, target_library_id, 
                                              priority_order, similarity_threshold)
            
            print(f"ğŸ¯ ç¬¬ä¸€å±‚æœç´¢ç»“æœ: {len(results)} ä¸ªåŒ¹é…")
            
            # å¦‚æœå¿«é€Ÿæœç´¢è¿”å›0ä¸ªç»“æœï¼Œè¿›è¡Œç¬¬äºŒå±‚ç²¾ç¡®æœç´¢
            if len(results) == 0:
                print("ï¿½ ï¿½ğŸ“¢ ç¬¬ä¸€å±‚æœªæ‰¾åˆ°ç»“æœï¼Œå¯åŠ¨ç¬¬äºŒå±‚ç²¾ç¡®æœç´¢...")
                results = self._perform_precise_search(source_material, target_library_id, 
                                                     priority_order, similarity_threshold)
                print(f"ğŸ¯ ç¬¬äºŒå±‚æœç´¢ç»“æœ: {len(results)} ä¸ªåŒ¹é…")
            else:
                print(f"âœ… ç¬¬ä¸€å±‚æ‰¾åˆ°è¶³å¤Ÿç»“æœï¼Œè·³è¿‡ç¬¬äºŒå±‚æœç´¢")
            
            print(f"ğŸ === ä¸¤å±‚æœç´¢å®Œæˆï¼Œæœ€ç»ˆç»“æœ: {len(results)} ä¸ª === ğŸ")
            return results
            
        except Exception as e:
            raise Exception(f"æè´¨åŒ¹é…å¤±è´¥: {str(e)}")
    
    def _perform_fast_search(self, source_material: Dict, target_library_id: int, 
                           priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ç¬¬ä¸€å±‚ï¼šå¿«é€Ÿé¢„ç­›é€‰æœç´¢
        """
        try:
            # è·å–ç›®æ ‡åº“ä¸­çš„æ‰€æœ‰æè´¨
            target_materials = self.database_manager.get_materials_by_library(target_library_id)
            
            print(f"å¿«é€Ÿé¢„ç­›é€‰: å¤„ç† {len(target_materials)} ä¸ªæè´¨...")
            
            # è·å–æºæè´¨çš„è¯¦ç»†ä¿¡æ¯
            source_details = self._get_material_details(source_material)
            
            results = []
            processed_count = 0
            progress_interval = max(1, len(target_materials) // 20)  # æ¯5%æ˜¾ç¤ºè¿›åº¦
            
            # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„ä¼˜å…ˆçº§æƒé‡
            weights = self._calculate_weights(priority_order)
            library_name = self._get_library_name(target_library_id)
            
            start_time = time.time()
            
            # é¢„ç­›é€‰ç»Ÿè®¡
            prefilter_passed = 0
            prefilter_total = 0
            
            for target_material in target_materials:
                try:
                    processed_count += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        progress = (processed_count / len(target_materials)) * 100
                        rate = processed_count / elapsed if elapsed > 0 else 0
                        print(f"å¿«é€Ÿé¢„ç­›é€‰è¿›åº¦: {progress:.1f}% ({processed_count}/{len(target_materials)}) - é€Ÿåº¦: {rate:.0f}æè´¨/ç§’")
                    
                    # è·³è¿‡åŒä¸€ä¸ªæè´¨
                    if (source_material.get('id') == target_material.get('id') and 
                        source_material.get('library_id') == target_material.get('library_id')):
                        continue
                    
                    # ä¸¥æ ¼çš„å¿«é€Ÿé¢„ç­›é€‰ - åªé€šè¿‡æ˜æ˜¾ç›¸å…³çš„æè´¨
                    prefilter_total += 1
                    prefilter_result = self._strict_prefilter(source_details, target_material, similarity_threshold)
                    
                    # è°ƒè¯•å‰å‡ ä¸ªæè´¨çš„é¢„ç­›é€‰ç»“æœ
                    if prefilter_total <= 5:
                        target_name = target_material.get('filename', 'Unknown')
                        print(f"  è°ƒè¯•é¢„ç­›é€‰ {prefilter_total}: {target_name} -> {'é€šè¿‡' if prefilter_result else 'è¢«æ’é™¤'}")
                    
                    if not prefilter_result:
                        continue
                    prefilter_passed += 1
                    
                    # è®¡ç®—è¯¦ç»†ç›¸ä¼¼åº¦
                    target_details = self._get_material_details(target_material)
                    similarity_info = self._calculate_similarity_optimized(
                        source_details, 
                        target_details, 
                        weights
                    )
                    
                    total_similarity = similarity_info['total']
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
                    if total_similarity >= similarity_threshold:
                        results.append({
                            'material': target_material,
                            'similarity': total_similarity,
                            'details': similarity_info['details'],
                            'library_name': library_name
                        })
                        
                        # æ—©æœŸé€€å‡º
                        if len(results) >= self.max_results_per_search:
                            print(f"å¿«é€Ÿé¢„ç­›é€‰æ‰¾åˆ° {self.max_results_per_search} ä¸ªåŒ¹é…ç»“æœï¼Œåœæ­¢æœç´¢")
                            break
                        
                except Exception as e:
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            elapsed = time.time() - start_time
            prefilter_rate = (prefilter_passed / prefilter_total * 100) if prefilter_total > 0 else 0
            print(f"å¿«é€Ÿé¢„ç­›é€‰å®Œæˆ: å¤„ç†äº†{processed_count}ä¸ªæè´¨ï¼Œé¢„ç­›é€‰é€šè¿‡{prefilter_passed}/{prefilter_total}({prefilter_rate:.1f}%)ï¼Œæ‰¾åˆ°{len(results)}ä¸ªåŒ¹é…ç»“æœï¼Œè€—æ—¶{elapsed:.1f}ç§’")
            
            return results
            
        except Exception as e:
            raise Exception(f"å¿«é€Ÿé¢„ç­›é€‰å¤±è´¥: {str(e)}")
    
    def _perform_precise_search(self, source_material: Dict, target_library_id: int, 
                              priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ç¬¬äºŒå±‚ï¼šç²¾ç¡®æœç´¢ - ä»å¤´å¼€å§‹æœç´¢å…¨éƒ¨æè´¨ï¼Œé™ä½é˜ˆå€¼ä»¥æ‰¾åˆ°æ½œåœ¨åŒ¹é…
        """
        print("ğŸ”¥ å¯åŠ¨ç¬¬äºŒå±‚ç²¾ç¡®æœç´¢ - ä»å¤´å¼€å§‹æœç´¢å…¨éƒ¨æè´¨...")
        
        # ç¬¬äºŒå±‚æœç´¢å…³é”®ç­–ç•¥ï¼š
        # 1. é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ä»¥æ‰¾åˆ°æ›´å¤šæ½œåœ¨åŒ¹é…
        # 2. ä»å…¨éƒ¨æè´¨å¼€å§‹æœç´¢ï¼Œä¸åšä»»ä½•é¢„ç­›é€‰
        # 3. ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„æƒé‡è¿›è¡Œç²¾ç¡®è®¡ç®—
        
        # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ - ä»åŸé˜ˆå€¼é™åˆ°æ›´å®½æ¾çš„æ°´å¹³
        relaxed_threshold = max(0.1, similarity_threshold * 0.3)  # é™åˆ°åŸæ¥çš„30%ï¼Œæœ€ä½10%
        print(f"ğŸ¯ é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}% -> {relaxed_threshold}%")
        
        try:
            # ç›´æ¥ä½¿ç”¨å•çº¿ç¨‹è¿›è¡Œå…¨é¢æœç´¢ï¼Œé¿å…å¤šçº¿ç¨‹å¤æ‚æ€§
            results = self._single_thread_precise_search(source_material, target_library_id, 
                                                       priority_order, relaxed_threshold)
            print(f"ğŸŠ ç¬¬äºŒå±‚ç²¾ç¡®æœç´¢å®Œæˆ: æ‰¾åˆ°{len(results)}ä¸ªåŒ¹é…ç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ ç²¾ç¡®æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _single_thread_precise_search(self, source_material: Dict, target_library_id: int, 
                                    priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        å•çº¿ç¨‹ç²¾ç¡®æœç´¢ - ä»å¤´å¼€å§‹æœç´¢å…¨éƒ¨æè´¨ï¼Œä¸ä½¿ç”¨ä»»ä½•é¢„ç­›é€‰
        """
        print("ğŸ” ç¬¬äºŒå±‚ç²¾ç¡®æœç´¢ï¼šä»å¤´å¼€å§‹æœç´¢å…¨éƒ¨æè´¨ï¼Œä¸ä½¿ç”¨ä»»ä½•é¢„ç­›é€‰...")
        
        target_materials = self.database_manager.get_materials_by_library(target_library_id)
        print(f"ğŸ“Š ç›®æ ‡æè´¨åº“æ€»æ•°: {len(target_materials)} ä¸ªæè´¨")
        
        source_details = self._get_material_details(source_material)
        weights = self._calculate_weights(priority_order)  # ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„å®Œæ•´æƒé‡
        library_name = self._get_library_name(target_library_id)
        
        results = []
        processed_count = 0
        start_time = time.time()
        progress_interval = max(100, len(target_materials) // 20)  # æ˜¾ç¤º20æ¬¡è¿›åº¦
        
        print(f"ğŸ’¡ ä½¿ç”¨æƒé‡é…ç½®: {weights}")
        print(f"ğŸ¯ ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        for target_material in target_materials:
            try:
                processed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if processed_count % progress_interval == 0:
                    elapsed = time.time() - start_time
                    progress = (processed_count / len(target_materials)) * 100
                    rate = processed_count / elapsed if elapsed > 0 else 0
                    print(f"ğŸ”„ ç²¾ç¡®æœç´¢è¿›åº¦: {progress:.1f}% ({processed_count}/{len(target_materials)}) - é€Ÿåº¦: {rate:.0f}æè´¨/ç§’")
                
                # è·³è¿‡åŒä¸€ä¸ªæè´¨
                if (source_material.get('id') == target_material.get('id') and 
                    source_material.get('library_id') == target_material.get('library_id')):
                    continue
                
                # ğŸš¨ å…³é”®ï¼šç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç»å¯¹ä¸ä½¿ç”¨ä»»ä½•é¢„ç­›é€‰ï¼
                target_details = self._get_material_details(target_material)
                similarity_info = self._calculate_similarity_optimized(
                    source_details, 
                    target_details, 
                    weights
                )
                
                total_similarity = similarity_info['total']
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
                if total_similarity >= similarity_threshold:
                    results.append({
                        'material': target_material,
                        'similarity': total_similarity,
                        'details': similarity_info['details'],
                        'library_name': library_name
                    })
                    
                    # æ˜¾ç¤ºæ‰¾åˆ°çš„åŒ¹é…
                    if len(results) <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªåŒ¹é…
                        target_name = target_material.get('filename', 'Unknown')
                        print(f"  âœ… æ‰¾åˆ°åŒ¹é… #{len(results)}: {target_name} (ç›¸ä¼¼åº¦: {total_similarity:.1f}%)")
                        
            except Exception as e:
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        elapsed = time.time() - start_time
        print(f"âœ… ç¬¬äºŒå±‚ç²¾ç¡®æœç´¢å®Œæˆ: å¤„ç†{processed_count}ä¸ªæè´¨ï¼Œæ‰¾åˆ°{len(results)}ä¸ªåŒ¹é…ç»“æœï¼Œè€—æ—¶{elapsed:.1f}ç§’")
        
        return results
    
    def _strict_prefilter(self, source_details: Dict, target_material: Dict, threshold: float) -> bool:
        """
        ä¸¥æ ¼é¢„ç­›é€‰ - åªé€šè¿‡æ˜æ˜¾ç›¸å…³çš„æè´¨ï¼Œé¿å…æ— å…³æè´¨å¹²æ‰°
        """
        try:
            # ä¸¥æ ¼é¢„ç­›é€‰ï¼šæ£€æŸ¥æè´¨åç§°ã€ç€è‰²å™¨è·¯å¾„ã€é‡‡æ ·å™¨ç±»å‹çš„åŸºæœ¬ç›¸ä¼¼æ€§
            
            # 1. æ£€æŸ¥æè´¨åç§°ç›¸ä¼¼æ€§
            source_filename = source_details.get('filename', '').lower()
            target_filename = target_material.get('filename', '').lower()
            
            # æå–æ–‡ä»¶åçš„ä¸»è¦éƒ¨åˆ†ï¼ˆå»æ‰æ‰©å±•åï¼‰
            source_name = source_filename.replace('.matbin', '').replace('.xml', '')
            target_name = target_filename.replace('.matbin', '').replace('.xml', '')
            
            # æ£€æŸ¥åç§°ç›¸ä¼¼æ€§
            name_similarity = self._calculate_name_similarity(source_name, target_name)
            
            # 2. æ£€æŸ¥ç€è‰²å™¨è·¯å¾„ç›¸ä¼¼æ€§
            source_shader = source_details.get('shader_path', '')
            target_shader = target_material.get('shader_path', '')
            shader_similarity = self._calculate_shader_similarity(source_shader, target_shader)
            
            # 3. æ£€æŸ¥é‡‡æ ·å™¨ç±»å‹ç›¸ä¼¼æ€§
            sampler_similarity = self._calculate_sampler_type_similarity(source_details, target_material)
            
            # 4. ç»„åˆåˆ¤æ–­ - è‡³å°‘ä¸€ä¸ªç»´åº¦è¦æœ‰ä¸€å®šç›¸ä¼¼æ€§
            min_name_threshold = 0.3      # åç§°ç›¸ä¼¼åº¦è‡³å°‘30%
            min_shader_threshold = 0.2    # ç€è‰²å™¨ç›¸ä¼¼åº¦è‡³å°‘20%
            min_sampler_threshold = 0.15  # é‡‡æ ·å™¨ç±»å‹ç›¸ä¼¼åº¦è‡³å°‘15%
            
            # ä»»ä½•ä¸€ä¸ªç»´åº¦è¾¾æ ‡å°±é€šè¿‡
            if (name_similarity >= min_name_threshold or 
                shader_similarity >= min_shader_threshold or 
                sampler_similarity >= min_sampler_threshold):
                return True
            
            # 5. ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœé˜ˆå€¼å¾ˆä½ï¼ˆ<=15%ï¼‰ï¼Œæ”¾å®½é™åˆ¶
            if threshold <= 15.0:
                if (name_similarity >= 0.1 or 
                    shader_similarity >= 0.1 or 
                    sampler_similarity >= 0.05):
                    return True
            
            return False
            
        except Exception as e:
            # é¢„ç­›é€‰å¤±è´¥æ—¶ä¿å®ˆé€šè¿‡
            return True
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """
        è®¡ç®—æè´¨åç§°çš„ç›¸ä¼¼åº¦
        """
        if not name1 or not name2:
            return 0.0
        
        # ç®€å•çš„å­ä¸²åŒ¹é…
        if name1 == name2:
            return 1.0
        
        if name1 in name2 or name2 in name1:
            return 0.8
        
        # æ£€æŸ¥å…±åŒçš„è¯æ±‡éƒ¨åˆ†
        words1 = set(name1.lower().replace('_', ' ').replace('[', ' ').replace(']', ' ').split())
        words2 = set(name2.lower().replace('_', ' ').replace('[', ' ').replace(']', ' ').split())
        
        if not words1 or not words2:
            return 0.0
        
        common_words = words1.intersection(words2)
        total_words = words1.union(words2)
        
        return len(common_words) / len(total_words) if total_words else 0.0
    
    def _calculate_shader_similarity(self, shader1: str, shader2: str) -> float:
        """
        è®¡ç®—ç€è‰²å™¨è·¯å¾„çš„ç›¸ä¼¼åº¦
        """
        if not shader1 or not shader2:
            return 0.0
        
        if shader1 == shader2:
            return 1.0
        
        # æå–ç€è‰²å™¨åç§°ï¼ˆæœ€åçš„æ–‡ä»¶åéƒ¨åˆ†ï¼‰
        name1 = shader1.split('\\')[-1].split('/')[-1].lower()
        name2 = shader2.split('\\')[-1].split('/')[-1].lower()
        
        if name1 == name2:
            return 0.9
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„ç€è‰²å™¨ç±»å‹å…³é”®è¯
        shader_types = ['detailblend', 'amsn', 'cloth', 'metal', 'skin', 'hair']
        type1 = None
        type2 = None
        
        for shader_type in shader_types:
            if shader_type in name1.lower():
                type1 = shader_type
            if shader_type in name2.lower():
                type2 = shader_type
        
        if type1 and type2 and type1 == type2:
            return 0.6
        
        return 0.0
    
    def _calculate_sampler_type_similarity(self, source_details: Dict, target_material: Dict) -> float:
        """
        è®¡ç®—é‡‡æ ·å™¨ç±»å‹çš„ç›¸ä¼¼åº¦
        """
        try:
            # è·å–æºæè´¨çš„é‡‡æ ·å™¨ç±»å‹
            source_samplers = source_details.get('samplers', [])
            if not source_samplers:
                return 0.0
            
            # éœ€è¦ä»target_materialè·å–é‡‡æ ·å™¨ä¿¡æ¯
            target_details = self._get_material_details(target_material)
            target_samplers = target_details.get('samplers', [])
            
            if not target_samplers:
                return 0.0
            
            # æå–é‡‡æ ·å™¨ç±»å‹å…³é”®è¯
            source_types = set()
            target_types = set()
            
            for sampler in source_samplers:
                sampler_type = sampler.get('type', '').lower()
                if sampler_type:
                    # æå–å…³é”®è¯ï¼šAlbedoMap, NormalMap, MetallicMapç­‰
                    for keyword in ['albedo', 'normal', 'metallic', 'roughness', 'specular', 'diffuse', 'ao', 'height', 'emission']:
                        if keyword in sampler_type:
                            source_types.add(keyword)
                    
                    # æå–çº¹ç†ç±»å‹ï¼šTexture2D, TextureCubeç­‰
                    if 'texture2d' in sampler_type:
                        source_types.add('texture2d')
                    elif 'texturecube' in sampler_type:
                        source_types.add('texturecube')
            
            for sampler in target_samplers:
                sampler_type = sampler.get('type', '').lower()
                if sampler_type:
                    # æå–å…³é”®è¯
                    for keyword in ['albedo', 'normal', 'metallic', 'roughness', 'specular', 'diffuse', 'ao', 'height', 'emission']:
                        if keyword in sampler_type:
                            target_types.add(keyword)
                    
                    # æå–çº¹ç†ç±»å‹
                    if 'texture2d' in sampler_type:
                        target_types.add('texture2d')
                    elif 'texturecube' in sampler_type:
                        target_types.add('texturecube')
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            if not source_types and not target_types:
                return 0.5  # éƒ½æ²¡æœ‰é‡‡æ ·å™¨ç±»å‹ï¼Œç»™ä¸ªä¸­ç­‰åˆ†æ•°
            
            if not source_types or not target_types:
                return 0.0
            
            # Jaccardç›¸ä¼¼åº¦
            intersection = source_types.intersection(target_types)
            union = source_types.union(target_types)
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception as e:
            # è®¡ç®—å¤±è´¥æ—¶è¿”å›0ï¼Œè®©å…¶ä»–ç»´åº¦èµ·ä½œç”¨
            return 0.0
    
    def _extract_common_shader_keywords(self, shader_path: str) -> List[str]:
        """
        ä»ç€è‰²å™¨è·¯å¾„æå–é€šç”¨å…³é”®è¯ï¼Œå¿½ç•¥ç‰¹æ®Šçš„DLCæˆ–è§’è‰²åç§°
        """
        if not shader_path:
            return []
        
        # è½¬æ¢ä¸ºå°å†™å¹¶åˆ†å‰²è·¯å¾„
        path_lower = shader_path.lower().replace('\\', '/')
        parts = [part for part in path_lower.split('/') if part]
        
        # é€šç”¨æè´¨ç±»å‹å…³é”®è¯
        common_keywords = []
        material_types = ['cloth', 'metal', 'skin', 'hair', 'fabric', 'leather', 'wood', 'stone', 'glass', 'plastic']
        shader_types = ['hlsl', 'spx', 'shader', 'mtl', 'mat']
        path_indicators = ['shaders', 'material', 'materials', 'outputdata', 'sat']
        
        # æå–æè´¨ç±»å‹å…³é”®è¯
        for part in parts:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æè´¨ç±»å‹
            for mat_type in material_types:
                if mat_type in part:
                    common_keywords.append(mat_type)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç€è‰²å™¨ç±»å‹
            for shader_type in shader_types:
                if part.endswith(f'.{shader_type}') or shader_type in part:
                    common_keywords.append(shader_type)
                    
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„æŒ‡ç¤ºè¯
            for indicator in path_indicators:
                if indicator in part:
                    common_keywords.append(indicator)
        
        # å»é‡å¹¶è¿”å›
        return list(set(common_keywords))
    

    
    def _calculate_similarity_optimized(self, source_details: Dict, target_details: Dict, 
                                      weights: Dict[str, float]) -> Dict:
        """
        ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³• - ç›´æ¥ä½¿ç”¨åŸå§‹æƒé‡ï¼Œä¸åšä»»ä½•ä¿®æ”¹
        """
        # ç›´æ¥è°ƒç”¨çˆ¶ç±»æ–¹æ³•ï¼Œä¸åšä»»ä½•æƒé‡è°ƒæ•´æˆ–ä¿æŠ¤
        return super()._calculate_similarity_optimized(source_details, target_details, weights)
    
    def _calculate_fast_weights_from_priority(self, priority_order: List[str]) -> Dict[str, float]:
        """
        åŸºäºç”¨æˆ·ä¼˜å…ˆçº§è®¡ç®—å¿«é€ŸåŒ¹é…æƒé‡ - åªå…³æ³¨å…³é”®å±æ€§ä½†å°Šé‡ç”¨æˆ·ä¼˜å…ˆçº§
        """
        # å¿«é€ŸåŒ¹é…åªå…³æ³¨è¿™ä¸‰ä¸ªå…³é”®å±æ€§
        fast_features = ['material_keywords', 'shader_path', 'sampler_types']
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆçº§æˆ–ä¼˜å…ˆçº§ä¸­æ²¡æœ‰å…³é”®å±æ€§ï¼Œä½¿ç”¨é»˜è®¤å¿«é€Ÿæƒé‡
        if not priority_order or not any(feature in fast_features for feature in priority_order):
            return self._calculate_fast_weights()
        
        weights = {}
        
        # åªä¸ºå¿«é€ŸåŒ¹é…çš„å…³é”®å±æ€§åˆ†é…æƒé‡
        for feature in fast_features:
            weights[feature] = 0.0
        
        # æ ¹æ®ç”¨æˆ·ä¼˜å…ˆçº§åˆ†é…æƒé‡
        total_priority_features = 0
        for feature in priority_order:
            if feature in fast_features:
                total_priority_features += 1
        
        if total_priority_features > 0:
            # æ ¹æ®ä¼˜å…ˆçº§é¡ºåºåˆ†é…æƒé‡
            remaining_weight = 1.0
            for i, feature in enumerate(priority_order):
                if feature in fast_features:
                    # ä¼˜å…ˆçº§è¶Šé«˜ï¼Œæƒé‡è¶Šå¤§
                    priority_weight = remaining_weight * 0.6  # æ¯ä¸ªçº§åˆ«åˆ†é…60%çš„å‰©ä½™æƒé‡
                    weights[feature] = priority_weight
                    remaining_weight *= 0.4
            
            # é‡æ–°è§„èŒƒåŒ–æƒé‡
            total_weight = sum(weights.values())
            if total_weight > 0:
                for feature in weights:
                    weights[feature] = weights[feature] / total_weight
        else:
            # å¦‚æœä¼˜å…ˆçº§ä¸­æ²¡æœ‰å¿«é€ŸåŒ¹é…å…³é”®å±æ€§ï¼Œä½¿ç”¨å‡ç­‰æƒé‡
            for feature in fast_features:
                weights[feature] = 1.0 / len(fast_features)
        
        # å…¶ä»–å±æ€§æƒé‡ä¸º0ï¼ˆå¿«é€ŸåŒ¹é…å¿½ç•¥ï¼‰
        weights['sampler_paths'] = 0.0
        weights['parameters'] = 0.0
        weights['sampler_count'] = 0.0
        
        return weights
    
    def _calculate_fast_weights(self) -> Dict[str, float]:
        """
        è®¡ç®—å¿«é€ŸåŒ¹é…ä¸“ç”¨æƒé‡ - ä¼˜å…ˆåç§°ã€ç€è‰²å™¨è·¯å¾„ã€é‡‡æ ·å™¨ç±»å‹
        """
        return {
            'material_keywords': 0.35,    # æè´¨åç§°æƒé‡æœ€é«˜
            'shader_path': 0.30,          # ç€è‰²å™¨è·¯å¾„æƒé‡å…¶æ¬¡ 
            'sampler_types': 0.20,        # é‡‡æ ·å™¨ç±»å‹æƒé‡ç¬¬ä¸‰
            'sampler_paths': 0.10,        # é‡‡æ ·å™¨è·¯å¾„æƒé‡è¾ƒä½
            'parameters': 0.05,           # å‚æ•°æƒé‡æœ€ä½
            'sampler_count': 0.0          # é‡‡æ ·å™¨æ•°é‡æƒé‡ä¸º0ï¼ˆå¿«é€Ÿæ¨¡å¼å¿½ç•¥ï¼‰
        }