#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè´¨åŒ¹é…å™¨æ ¸å¿ƒæ¨¡å—
Material Matcher Core Module
"""

import re
import json
from typing import Dict, List, Tuple, Optional, Any
from difflib import SequenceMatcher
from difflib import SequenceMatcher
import math

class MaterialMatcher:
    """æè´¨åŒ¹é…å™¨"""
    
    def __init__(self, database_manager):
        self.database_manager = database_manager
        
        # æ·»åŠ ç¼“å­˜ä»¥å‡å°‘æ•°æ®åº“æŸ¥è¯¢
        self._library_cache = {}
        self._material_details_cache = {}
        
        # åŒ¹é…æƒé‡é…ç½®
        self.default_weights = {
            'sampler_types': 0.30,      # é‡‡æ ·å™¨ç±»å‹
            'shader_path': 0.25,        # ç€è‰²å™¨è·¯å¾„
            'sampler_count': 0.15,      # é‡‡æ ·å™¨æ•°é‡
            'parameters': 0.15,         # å¯ç¼–è¾‘å‚æ•°
            'material_keywords': 0.10,   # æè´¨åç§°å…³é”®è¯
            'sampler_paths': 0.05        # é‡‡æ ·å™¨è·¯å¾„
        }
        
        # å…³é”®è¯æå–æ­£åˆ™è¡¨è¾¾å¼
        self.keyword_patterns = {
            'sampler_types': [
                r'(?i)([A-Z]+)(?=_)',           # å¤§å†™å­—æ¯å¼€å¤´çš„å‰ç¼€
                r'(?i)(MetallicMap|NormalMap|DiffuseMap|SpecularMap|RoughnessMap|AOMap|HeightMap|EmissiveMap)',
                r'(?i)(Texture2D|TextureCube|Texture3D)',
                r'(?i)(AMSN|AMSO|AMSS|AMSB)',
                r'(?i)(Mb\d+)',                 # Mbåè·Ÿæ•°å­—
            ],
            'shader_path': [
                r'(?i)(cloth|hair|metal|skin|glass|fabric|leather|wood|stone|water)',
                r'(?i)(DetailBlend|LayerBlend|MultiLayer)',
                r'(?i)(PBR|Phong|Lambert|Blinn)',
            ],
            'material_keywords': [
                r'(?i)(hair|cloth|metal|skin|fabric|leather|wood|stone|glass|water|plastic)',
                r'(?i)(rough|smooth|glossy|matte|transparent|opaque)',
                r'(?i)(female|male|character|environment|prop)',
            ]
        }
    
    def find_similar_materials(self, source_material: Dict, target_library_id: int, 
                              priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ç²¾ç¡®æœç´¢ - ä½¿ç”¨ä¸¤å±‚æœç´¢ç­–ç•¥ç¡®ä¿æ‰¾åˆ°ç›¸ä¼¼æè´¨
        
        Args:
            source_material: æºæè´¨ä¿¡æ¯
            target_library_id: ç›®æ ‡åº“ID
            priority_order: åŒ¹é…ä¼˜å…ˆçº§é¡ºåº
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«æè´¨ä¿¡æ¯ã€ç›¸ä¼¼åº¦å’Œè¯¦ç»†åŒ¹é…ä¿¡æ¯
        """
        # ç²¾ç¡®æœç´¢ï¼šä¸¤å±‚æœç´¢ç­–ç•¥ï¼ˆé™é»˜æ¨¡å¼ï¼‰
        
        # ç¬¬ä¸€å±‚ï¼šå¸¦é¢„ç­›é€‰çš„å¿«é€Ÿæœç´¢
        results = self._perform_prefiltered_search(source_material, target_library_id, 
                                                  priority_order, similarity_threshold)
        
        # å¦‚æœç¬¬ä¸€å±‚è¿”å›0ä¸ªç»“æœï¼Œè¿›è¡Œç¬¬äºŒå±‚å…¨é¢æœç´¢
        if len(results) == 0:
            results = self._perform_comprehensive_search(source_material, target_library_id, 
                                                        priority_order, similarity_threshold)
        
        return results
    
    def _perform_prefiltered_search(self, source_material: Dict, target_library_id: int, 
                                   priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ç¬¬ä¸€å±‚ï¼šå¸¦é¢„ç­›é€‰çš„å¿«é€Ÿæœç´¢
        """
        try:
            # è·å–ç›®æ ‡åº“ä¸­çš„æ‰€æœ‰æè´¨
            target_materials = self.database_manager.get_materials_by_library(target_library_id)
            
            # è·å–æºæè´¨çš„è¯¦ç»†ä¿¡æ¯
            source_details = self._get_material_details(source_material)
            
            results = []
            processed_count = 0
            prefilter_passed = 0
            progress_interval = max(100, len(target_materials) // 20)  # æ˜¾ç¤º20æ¬¡è¿›åº¦
            
            # é¢„è®¡ç®—æƒé‡å’Œåº“åç§°ï¼Œé¿å…é‡å¤è®¡ç®—
            weights = self._calculate_weights(priority_order)
            library_name = self._get_library_name(target_library_id)
            
            # ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿé¢„ç­›é€‰ï¼Œåªæ”¶é›†å€™é€‰æè´¨
            candidate_materials = []
            
            for target_material in target_materials:
                try:
                    processed_count += 1
                    
                    # é™é»˜è¿›åº¦
                    
                    # è·³è¿‡åŒä¸€ä¸ªæè´¨
                    if (source_material.get('id') == target_material.get('id') and 
                        source_material.get('library_id') == target_material.get('library_id')):
                        continue
                    
                    # å¿«é€Ÿé¢„ç­›é€‰ - åªåšç®€å•çš„åŒ¹é…åˆ¤æ–­
                    if self._quick_prefilter(source_details, target_material, similarity_threshold):
                        candidate_materials.append(target_material)
                        prefilter_passed += 1
                        
                except Exception as e:
                    continue
            
            # ç¬¬ä¸€å±‚é¢„ç­›é€‰å®Œæˆï¼ˆé™é»˜ï¼‰
            
            # å¦‚æœå€™é€‰æè´¨æ•°é‡ä¸º0ï¼Œç›´æ¥è¿”å›ç©ºç»“æœï¼ˆå°†è§¦å‘ç¬¬äºŒå±‚æœç´¢ï¼‰
            if len(candidate_materials) == 0:
                return results
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¯¹å€™é€‰æè´¨è¿›è¡Œå®Œæ•´ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆé™é»˜ï¼‰
            
            for i, target_material in enumerate(candidate_materials):
                try:
                    
                    # è·å–ç›®æ ‡æè´¨è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè®¡ç®—æ•°é‡ï¼‰
                    target_details = self._get_material_details(target_material)
                    
                    # è®¡ç®—è¯¦ç»†ç›¸ä¼¼åº¦
                    similarity_info = self._calculate_similarity_optimized(
                        source_details, 
                        target_material, 
                        weights
                    )
                    
                    total_similarity = similarity_info['total']
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
                    if total_similarity >= similarity_threshold:
                        # å°†é‡‡æ ·å™¨/å‚æ•°ä¿¡æ¯æ·»åŠ åˆ°è¯¦æƒ…ä¸­ï¼Œä¾¿äºUIæ˜¾ç¤º
                        details = similarity_info['details'].copy()
                        details['source_sampler_count'] = len(source_details.get('samplers', []))
                        details['target_sampler_count'] = len(target_details.get('samplers', []))
                        details['source_param_count'] = len(source_details.get('parameters', []))
                        details['target_param_count'] = len(target_details.get('parameters', []))
                        
                        results.append({
                            'material': target_material,
                            'similarity': total_similarity,
                            'details': details,
                            'library_name': library_name,
                            'source_material': source_material,  # æ·»åŠ æºæè´¨ä¿¡æ¯
                            'target_material': target_material   # æ·»åŠ ç›®æ ‡æè´¨ä¿¡æ¯
                        })
                        
                        # æ—©æœŸé€€å‡ºï¼šå¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„ç»“æœå°±åœæ­¢
                        if len(results) >= 1000:  # æœ€å¤š1000ä¸ªç»“æœ
                            break
                        
                except Exception as e:
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            return results
            
        except Exception as e:
            return []
    
    def _perform_comprehensive_search(self, source_material: Dict, target_library_id: int, 
                                     priority_order: List[str], similarity_threshold: float) -> List[Dict]:
        """
        ç¬¬äºŒå±‚ï¼šå…¨é¢æœç´¢ - è·³è¿‡æ‰€æœ‰é¢„ç­›é€‰ï¼Œé™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        try:
            # ç¬¬äºŒå±‚å…¨é¢æœç´¢ï¼šè·³è¿‡æ‰€æœ‰é¢„ç­›é€‰ï¼Œé™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé™é»˜ï¼‰
            
            # è·å–ç›®æ ‡åº“ä¸­çš„æ‰€æœ‰æè´¨
            target_materials = self.database_manager.get_materials_by_library(target_library_id)
            

            
            # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆç”¨äºåˆç­›ï¼‰
            relaxed_threshold = max(10.0, similarity_threshold * 0.3)  # é™åˆ°åŸæ¥çš„30%ï¼Œæœ€ä½10%
            # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé™é»˜ï¼‰
            
            # è·å–æºæè´¨çš„è¯¦ç»†ä¿¡æ¯
            source_details = self._get_material_details(source_material)
            
            results = []
            processed_count = 0
            progress_interval = max(100, len(target_materials) // 20)  # æ˜¾ç¤º20æ¬¡è¿›åº¦
            
            # é¢„è®¡ç®—æƒé‡å’Œåº“åç§°
            weights = self._calculate_weights(priority_order)
            library_name = self._get_library_name(target_library_id)
            
            # é™é»˜æƒé‡å’Œé˜ˆå€¼é…ç½®
            
            for target_material in target_materials:
                try:
                    processed_count += 1
                    
                    # é™é»˜è¿›åº¦
                    
                    # è·³è¿‡åŒä¸€ä¸ªæè´¨
                    if (source_material.get('id') == target_material.get('id') and 
                        source_material.get('library_id') == target_material.get('library_id')):
                        continue
                    
                    # ğŸš¨ å…³é”®ï¼šç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç»å¯¹ä¸ä½¿ç”¨ä»»ä½•é¢„ç­›é€‰ï¼
                    target_details = self._get_material_details(target_material)
                    similarity_info = self._calculate_similarity_optimized(
                        source_details, 
                        target_details, 
                        weights
                    )
                    
                    total_similarity = similarity_info['total']
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ”¾å®½åçš„é˜ˆå€¼
                    if total_similarity >= relaxed_threshold:
                        # å°†é‡‡æ ·å™¨/å‚æ•°ä¿¡æ¯æ·»åŠ åˆ°è¯¦æƒ…ä¸­ï¼Œä¾¿äºUIæ˜¾ç¤º
                        details = similarity_info['details'].copy()
                        details['source_sampler_count'] = len(source_details.get('samplers', []))
                        details['target_sampler_count'] = len(target_details.get('samplers', []))
                        details['source_param_count'] = len(source_details.get('parameters', []))
                        details['target_param_count'] = len(target_details.get('parameters', []))
                        
                        results.append({
                            'material': target_material,
                            'similarity': total_similarity,
                            'details': details,
                            'library_name': library_name,
                            'source_material': source_material,  # æ·»åŠ æºæè´¨ä¿¡æ¯
                            'target_material': target_material   # æ·»åŠ ç›®æ ‡æè´¨ä¿¡æ¯
                        })
                        
                        # é™é»˜åŒ¹é…
                        
                except Exception as e:
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # ğŸš¨ é‡è¦ï¼šç”¨åŸå§‹é˜ˆå€¼å†è¿‡æ»¤ä¸€æ¬¡ï¼Œç¡®ä¿è¿”å›ç»“æœç¬¦åˆç”¨æˆ·è®¾ç½®
            filtered_results = [r for r in results if r['similarity'] >= similarity_threshold]
            
            return filtered_results
            
        except Exception as e:
            return []
    
    def _get_material_details(self, material: Dict) -> Dict:
        """è·å–æè´¨è¯¦ç»†ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        material_id = material.get('id')
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„idï¼Œé™é»˜ä¿®å¤
        if material_id is None or material_id == '' or material_id == 0:
            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ›¿ä»£IDï¼Œä¸è¾“å‡ºè­¦å‘Šï¼ˆè¿™ç§æƒ…å†µå¾ˆå°‘è§ä¸”ä¸å½±å“åŠŸèƒ½ï¼‰
            material_id = material.get('filename', material.get('file_name', f'temp_{hash(str(material))}'))
        
        # æ£€æŸ¥ç¼“å­˜
        if material_id in self._material_details_cache:
            return self._material_details_cache[material_id]
        
        details = {
            'basic_info': material,
            'samplers': [],
            'parameters': [],
            'shader_path': material.get('shader_path', '') or material.get('shader_name', '') or material.get('shader', ''),
            'keywords': [],
            'filename': material.get('filename', '') or material.get('file_name', '') or material.get('name', '')
        }
        
        # ä»æè´¨åç§°æå–å…³é”®è¯ï¼ˆä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”ï¼‰
        material_name = details['filename']
        if material_name:
            details['keywords'] = self._extract_material_keywords(material_name)
        
        try:
            # åªæœ‰å­˜åœ¨æœ‰æ•ˆIDæ—¶æ‰è·å–é‡‡æ ·å™¨ä¿¡æ¯
            if material.get('id'):
                samplers = self.database_manager.get_samplers(material['id'])
                if isinstance(samplers, list):
                    for sampler in samplers:
                        sampler_info = {
                            'name': sampler.get('name', ''),
                            'path': sampler.get('path', ''),
                            'type': sampler.get('type', ''),
                            'keywords': self._extract_keywords(sampler.get('type', ''), 'sampler_types')
                        }
                        details['samplers'].append(sampler_info)
                
                # è·å–å‚æ•°ä¿¡æ¯
                parameters = self.database_manager.get_parameters(material['id'])
                if isinstance(parameters, list):
                    for param in parameters:
                        param_info = {
                            'name': param.get('name', ''),
                            'type': param.get('type', ''),
                            'value': param.get('value', ''),
                            'default_value': param.get('default_value', '')
                        }
                        details['parameters'].append(param_info)
                
        except Exception as e:
            pass
            # å³ä½¿å‡ºé”™ä¹Ÿè¦è®¾ç½®åŸºæœ¬ä¿¡æ¯ï¼Œç¡®ä¿ç¨‹åºèƒ½ç»§ç»­è¿è¡Œ
        
        # ç¼“å­˜ç»“æœ
        if material_id:
            self._material_details_cache[material_id] = details
        
        return details
    
    def _calculate_similarity_optimized(self, source_details: Dict, target_material: Dict, 
                                      weights: Dict[str, float]) -> Dict:
        """ä¼˜åŒ–ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦è®¡ç®— - ä½¿ç”¨é¢„è®¡ç®—çš„æƒé‡"""
        
        # è·å–ç›®æ ‡æè´¨è¯¦ç»†ä¿¡æ¯
        target_details = self._get_material_details(target_material)
        
        # è®¡ç®—å„é¡¹åŒ¹é…åˆ†æ•°
        scores = {}
        
        try:
            # 1. é‡‡æ ·å™¨ç±»å‹åŒ¹é…
            source_samplers = source_details.get('samplers', [])
            target_samplers = target_details.get('samplers', [])
            
            if not isinstance(source_samplers, list):
                source_samplers = []
            if not isinstance(target_samplers, list):
                target_samplers = []
                
            scores['sampler_types'] = self._match_sampler_types(source_samplers, target_samplers)
            
            # 2. ç€è‰²å™¨è·¯å¾„åŒ¹é…
            source_shader = source_details.get('shader_path', '')
            target_shader = target_details.get('shader_path', '')
            scores['shader_path'] = self._match_shader_path(source_shader, target_shader)
            
            # 3. é‡‡æ ·å™¨æ•°é‡åŒ¹é…
            scores['sampler_count'] = self._match_sampler_count(len(source_samplers), len(target_samplers))
            
            # 4. å‚æ•°åŒ¹é…
            source_parameters = source_details.get('parameters', [])
            target_parameters = target_details.get('parameters', [])
            
            if not isinstance(source_parameters, list):
                source_parameters = []
            if not isinstance(target_parameters, list):
                target_parameters = []
                
            scores['parameters'] = self._match_parameters(source_parameters, target_parameters)
            
            # 5. æè´¨å…³é”®è¯åŒ¹é…ï¼ˆä½¿ç”¨æ–°çš„ä¸‹åˆ’çº¿åˆ†éš”ç®—æ³•ï¼‰
            source_keywords = source_details.get('keywords', [])
            target_keywords = target_details.get('keywords', [])
            
            if not isinstance(source_keywords, list):
                source_keywords = []
            if not isinstance(target_keywords, list):
                target_keywords = []
                
            scores['material_keywords'] = self._match_material_keywords(source_keywords, target_keywords)
            
            # 6. é‡‡æ ·å™¨è·¯å¾„åŒ¹é…
            scores['sampler_paths'] = self._match_sampler_paths(source_samplers, target_samplers)
            
            # ä½¿ç”¨é¢„è®¡ç®—çš„æƒé‡è®¡ç®—åŠ æƒæ€»åˆ†
            total_score = 0.0
            for feature, weight in weights.items():
                if feature in scores:
                    total_score += scores[feature] * weight
            
            # åº”ç”¨é—¨æ§›æƒ©ç½šæœºåˆ¶ï¼šæœ€é«˜ä¼˜å…ˆçº§ç‰¹å¾å¾—åˆ†ä½æ—¶æƒ©ç½šæ€»åˆ†
            total_score = self._apply_threshold_penalty(total_score, scores, weights)
            
            return {
                'total': total_score,
                'details': scores,
                'weights': weights
            }
            
        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            raise e
    
    def _calculate_similarity(self, source_details: Dict, target_material: Dict, 
                            priority_order: List[str]) -> Dict:
        """è®¡ç®—æè´¨ç›¸ä¼¼åº¦"""
        
        # è·å–ç›®æ ‡æè´¨è¯¦ç»†ä¿¡æ¯
        target_details = self._get_material_details(target_material)
        
        # è®¡ç®—å„é¡¹åŒ¹é…åˆ†æ•°
        scores = {}
        
        try:
            # 1. é‡‡æ ·å™¨ç±»å‹åŒ¹é…
            source_samplers = source_details.get('samplers', [])
            target_samplers = target_details.get('samplers', [])
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨ç±»å‹
            if not isinstance(source_samplers, list):
                print(f"è­¦å‘Š: source_samplers ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(source_samplers)}")
                source_samplers = []
            if not isinstance(target_samplers, list):
                print(f"è­¦å‘Š: target_samplers ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(target_samplers)}")
                target_samplers = []
                
            scores['sampler_types'] = self._match_sampler_types(
                source_samplers, 
                target_samplers
            )
            
            # 2. ç€è‰²å™¨è·¯å¾„åŒ¹é…
            source_shader = source_details.get('shader_path', '')
            target_shader = target_details.get('shader_path', '')
            
            scores['shader_path'] = self._match_shader_path(
                source_shader, 
                target_shader
            )
            
            # 3. é‡‡æ ·å™¨æ•°é‡åŒ¹é…
            scores['sampler_count'] = self._match_sampler_count(
                len(source_samplers), 
                len(target_samplers)
            )
            
            # 4. å‚æ•°åŒ¹é…
            source_parameters = source_details.get('parameters', [])
            target_parameters = target_details.get('parameters', [])
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨ç±»å‹
            if not isinstance(source_parameters, list):
                print(f"è­¦å‘Š: source_parameters ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(source_parameters)}")
                source_parameters = []
            if not isinstance(target_parameters, list):
                print(f"è­¦å‘Š: target_parameters ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(target_parameters)}")
                target_parameters = []
                
            scores['parameters'] = self._match_parameters(
                source_parameters, 
                target_parameters
            )
            
            # 5. æè´¨å…³é”®è¯åŒ¹é…ï¼ˆä½¿ç”¨æ–°çš„ä¸‹åˆ’çº¿åˆ†éš”ç®—æ³•ï¼‰
            source_keywords = source_details.get('keywords', [])
            target_keywords = target_details.get('keywords', [])
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨ç±»å‹
            if not isinstance(source_keywords, list):
                print(f"è­¦å‘Š: source_keywords ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(source_keywords)}")
                source_keywords = []
            if not isinstance(target_keywords, list):
                print(f"è­¦å‘Š: target_keywords ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(target_keywords)}")
                target_keywords = []
                
            scores['material_keywords'] = self._match_material_keywords(
                source_keywords, 
                target_keywords
            )
            
            # 6. é‡‡æ ·å™¨è·¯å¾„åŒ¹é…
            scores['sampler_paths'] = self._match_sampler_paths(
                source_samplers, 
                target_samplers
            )
            
            # æ ¹æ®ä¼˜å…ˆçº§è®¡ç®—åŠ æƒæ€»åˆ†
            total_score = 0.0
            weights = self._calculate_weights(priority_order)
            
            for feature, weight in weights.items():
                if feature in scores:
                    total_score += scores[feature] * weight
            
            return {
                'total': total_score,
                'details': scores,
                'weights': weights
            }
            
        except Exception as e:
            print(f"è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            print(f"source_detailsç±»å‹: {type(source_details)}")
            print(f"target_detailsç±»å‹: {type(target_details)}")
            print(f"priority_orderç±»å‹: {type(priority_order)}")
            raise e
    
    def _apply_threshold_penalty(self, total_score: float, scores: Dict[str, float], 
                                  weights: Dict[str, float]) -> float:
        """åº”ç”¨é—¨æ§›æƒ©ç½šæœºåˆ¶ï¼šæœ€é«˜æƒé‡ç‰¹å¾å¾—åˆ†ä½æ—¶æƒ©ç½šæ€»åˆ†"""
        if not scores or not weights:
            return total_score
        
        # æ‰¾åˆ°æƒé‡æœ€é«˜çš„ç‰¹å¾
        max_weight_feature = max(weights.keys(), key=lambda f: weights.get(f, 0))
        core_score = scores.get(max_weight_feature, 50)
        
        # åº”ç”¨æƒ©ç½š
        if core_score < 30:
            # æ ¸å¿ƒç‰¹å¾å¾—åˆ† < 30%ï¼Œä¸¥é‡æƒ©ç½šï¼ˆæ€»åˆ† Ã— 0.2~0.3ï¼‰
            penalty_factor = max(0.2, core_score / 100)
        elif core_score < 50:
            # æ ¸å¿ƒç‰¹å¾å¾—åˆ† < 50%ï¼Œä¸­ç­‰æƒ©ç½šï¼ˆæ€»åˆ† Ã— 0.5~0.6ï¼‰
            penalty_factor = max(0.5, core_score / 100)
        else:
            # æ ¸å¿ƒç‰¹å¾å¾—åˆ† >= 50%ï¼Œæ— æƒ©ç½š
            penalty_factor = 1.0
        
        return total_score * penalty_factor
    
    def _match_sampler_types(self, source_samplers: List[Dict], target_samplers: List[Dict]) -> float:
        """åŒ¹é…é‡‡æ ·å™¨ç±»å‹ - æ”¹è¿›ç‰ˆï¼šç±»å‹è¦†ç›–åº¦80% + å…³é”®è¯ç›¸ä¼¼åº¦20%"""
        if not source_samplers and not target_samplers:
            return 100.0
        if not source_samplers or not target_samplers:
            return 0.0
        
        # 1. æå–é‡‡æ ·å™¨ç±»å‹ç»Ÿè®¡ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªå…³é”®è¯ä½œä¸ºç±»å‹ï¼‰
        source_type_stats = self._get_sampler_type_stats(source_samplers)
        target_type_stats = self._get_sampler_type_stats(target_samplers)
        
        if not source_type_stats:
            return 50.0  # æºæ— æœ‰æ•ˆç±»å‹ï¼Œç»™ä¸­ç­‰åˆ†æ•°
        
        # 2. è®¡ç®—ç±»å‹è¦†ç›–åº¦ï¼ˆæºç±»å‹æ˜¯å¦è¢«ç›®æ ‡è¦†ç›–ï¼Œå…è®¸ç›®æ ‡æ›´å¤šï¼‰
        type_coverage_score = 0.0
        for sampler_type, source_count in source_type_stats.items():
            target_count = target_type_stats.get(sampler_type, 0)
            if target_count >= source_count:
                # ç›®æ ‡å®Œå…¨è¦†ç›–æºçš„è¯¥ç±»å‹
                type_coverage_score += 1.0
            elif target_count > 0:
                # éƒ¨åˆ†è¦†ç›–
                type_coverage_score += target_count / source_count
            # ç›®æ ‡æ²¡æœ‰è¯¥ç±»å‹ï¼š0åˆ†
        
        type_coverage = (type_coverage_score / len(source_type_stats)) * 100.0
        
        # 3. è®¡ç®—é‡‡æ ·å™¨å…³é”®è¯ç›¸ä¼¼åº¦ï¼ˆåŒ¹é…ç›¸åŒç±»å‹çš„é‡‡æ ·å™¨å¯¹ï¼‰
        keyword_similarity = self._calculate_sampler_keyword_similarity(
            source_samplers, target_samplers, source_type_stats, target_type_stats
        )
        
        # 4. ç»¼åˆå¾—åˆ†ï¼šç±»å‹è¦†ç›–80% + å…³é”®è¯ç›¸ä¼¼20%
        return type_coverage * 0.80 + keyword_similarity * 0.20
    
    def _get_sampler_type_stats(self, samplers: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡é‡‡æ ·å™¨ç±»å‹ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªå…³é”®è¯ä½œä¸ºç±»å‹ï¼‰"""
        type_stats = {}
        for sampler in samplers:
            sampler_type = self._extract_sampler_type(sampler)
            if sampler_type:
                type_stats[sampler_type] = type_stats.get(sampler_type, 0) + 1
        return type_stats
    
    def _extract_sampler_type(self, sampler: Dict) -> str:
        """æå–é‡‡æ ·å™¨ç±»å‹ï¼ˆæœ€åä¸€ä¸ª_åçš„å…³é”®è¯ï¼‰"""
        # ä¼˜å…ˆä» type å­—æ®µæå–ï¼Œå…¶æ¬¡ä» name å­—æ®µ
        sampler_name = sampler.get('type', '') or sampler.get('name', '')
        if not sampler_name:
            return ''
        
        # æŒ‰ _ åˆ†éš”ï¼Œå–æœ€åä¸€ä¸ªå…³é”®è¯ä½œä¸ºç±»å‹
        parts = sampler_name.split('_')
        if parts:
            return parts[-1]
        return sampler_name
    
    def _calculate_sampler_keyword_similarity(self, source_samplers: List[Dict], 
                                               target_samplers: List[Dict],
                                               source_type_stats: Dict[str, int],
                                               target_type_stats: Dict[str, int]) -> float:
        """è®¡ç®—é‡‡æ ·å™¨å…³é”®è¯ç›¸ä¼¼åº¦ï¼ˆè€ƒè™‘å®Œæ•´å…³é”®è¯é“¾ï¼‰"""
        if not source_samplers:
            return 100.0
        
        total_similarity = 0.0
        matched_count = 0
        
        for source_sampler in source_samplers:
            source_type = self._extract_sampler_type(source_sampler)
            if not source_type:
                continue
            
            # åœ¨ç›®æ ‡ä¸­æ‰¾åˆ°ç›¸åŒç±»å‹çš„é‡‡æ ·å™¨
            best_similarity = 0.0
            for target_sampler in target_samplers:
                target_type = self._extract_sampler_type(target_sampler)
                if target_type == source_type:
                    # è®¡ç®—å®Œæ•´å…³é”®è¯ç›¸ä¼¼åº¦
                    similarity = self._compare_sampler_keywords(source_sampler, target_sampler)
                    best_similarity = max(best_similarity, similarity)
            
            total_similarity += best_similarity
            matched_count += 1
        
        return (total_similarity / max(1, matched_count)) * 100.0
    
    def _compare_sampler_keywords(self, source_sampler: Dict, target_sampler: Dict) -> float:
        """æ¯”è¾ƒä¸¤ä¸ªé‡‡æ ·å™¨çš„å®Œæ•´å…³é”®è¯ç›¸ä¼¼åº¦"""
        source_name = source_sampler.get('type', '') or source_sampler.get('name', '')
        target_name = target_sampler.get('type', '') or target_sampler.get('name', '')
        
        if not source_name or not target_name:
            return 0.0
        
        # æå–æ‰€æœ‰å…³é”®è¯
        source_keywords = [kw.lower() for kw in source_name.split('_') if kw]
        target_keywords = [kw.lower() for kw in target_name.split('_') if kw]
        
        if not source_keywords:
            return 1.0
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦ï¼ˆæºå…³é”®è¯æœ‰å¤šå°‘åœ¨ç›®æ ‡ä¸­ï¼‰
        matched = sum(1 for kw in source_keywords if kw in target_keywords)
        return matched / len(source_keywords)
    
    def _match_shader_path(self, source_path: str, target_path: str) -> float:
        """åŒ¹é…ç€è‰²å™¨è·¯å¾„ - ä¿®å¤ç‰ˆæœ¬"""
        if not source_path and not target_path:
            return 100.0  # ä¸¤ä¸ªéƒ½ä¸ºç©ºï¼Œå®Œå…¨åŒ¹é…
        if not source_path or not target_path:
            return 0.0    # å…¶ä¸­ä¸€ä¸ªä¸ºç©ºï¼Œè¿”å›0åˆ†
        
        # é¦–å…ˆæ£€æŸ¥è·¯å¾„æ˜¯å¦å®Œå…¨ç›¸åŒ
        if source_path.lower().strip() == target_path.lower().strip():
            return 100.0  # å®Œå…¨ç›¸åŒï¼Œè¿”å›100%
        
        # è·¯å¾„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        text_similarity = SequenceMatcher(None, source_path.lower(), target_path.lower()).ratio()
        
        # å¦‚æœæ–‡æœ¬ç›¸ä¼¼åº¦å¾ˆé«˜ï¼ˆ>0.9ï¼‰ï¼Œç›´æ¥åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦è¯„åˆ†
        if text_similarity > 0.9:
            return text_similarity * 100.0
        
        # æå–è·¯å¾„å…³é”®è¯
        source_keywords = self._extract_keywords(source_path, 'shader_path')
        target_keywords = self._extract_keywords(target_path, 'shader_path')
        
        # å…³é”®è¯åŒ¹é…åº¦
        keyword_similarity = 0.0
        if source_keywords and target_keywords:
            common_keywords = set(source_keywords).intersection(set(target_keywords))
            max_keywords = max(len(source_keywords), len(target_keywords))
            keyword_similarity = len(common_keywords) / max_keywords
        elif not source_keywords and not target_keywords:
            # å¦‚æœéƒ½æ²¡æœ‰å…³é”®è¯ï¼ŒåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦
            keyword_similarity = text_similarity
        
        # ç»¼åˆè¯„åˆ†ï¼Œç»™æ–‡æœ¬ç›¸ä¼¼åº¦æ›´é«˜æƒé‡
        return (keyword_similarity * 0.3 + text_similarity * 0.7) * 100.0
    
    def _match_sampler_count(self, source_count: int, target_count: int) -> float:
        """åŒ¹é…é‡‡æ ·å™¨æ•°é‡ - æ”¹è¿›ç‰ˆï¼šå…è®¸ç›®æ ‡å¤šäºæº"""
        if source_count == 0 and target_count == 0:
            return 100.0
        if source_count == 0:
            return 50.0  # æºæ— é‡‡æ ·å™¨ï¼Œç›®æ ‡æœ‰ï¼Œç»™ä¸­ç­‰åˆ†
        
        # å…è®¸ç›®æ ‡å¤šäºæºï¼ˆæºæ˜¯å­é›†ï¼‰
        if target_count >= source_count:
            # ç›®æ ‡è¶Šæ¥è¿‘æºï¼Œåˆ†æ•°è¶Šé«˜ï¼›è¶…å‡ºè¶Šå¤šï¼Œé€‚å½“æ‰£åˆ†
            excess_ratio = (target_count - source_count) / source_count
            return max(50.0, 100.0 - excess_ratio * 30.0)  # è¶…å‡ºè¶Šå¤šæ‰£åˆ†ï¼Œæœ€ä½50
        else:
            # ç›®æ ‡å°‘äºæºï¼Œä¸¥é‡æ‰£åˆ†
            missing_ratio = (source_count - target_count) / source_count
            return max(0.0, 100.0 - missing_ratio * 100.0)  # ç¼ºå¤±å¤šå°‘æ‰£å¤šå°‘
    
    def _match_parameters(self, source_params: List[Dict], target_params: List[Dict]) -> float:
        """
        å…¨é¢åŒ¹é…å‚æ•° - è€ƒè™‘å‚æ•°åç§°åŒ¹é…å’Œå‚æ•°å€¼ç›¸ä¼¼åº¦
        
        è®¡ç®—é€»è¾‘:
        1. å‚æ•°åç§°åŒ¹é…åº¦ (æƒé‡40%)
        2. ç›¸åŒå‚æ•°åç§°çš„å‚æ•°å€¼ç›¸ä¼¼åº¦ (æƒé‡60%)
        """
        if not source_params and not target_params:
            return 100.0
        if not source_params or not target_params:
            return 0.0
        
        # å°†å‚æ•°è½¬æ¢ä¸ºå­—å…¸ï¼Œä¾¿äºæŒ‰åç§°æŸ¥æ‰¾
        source_dict = {param.get('name', ''): param for param in source_params if param.get('name')}
        target_dict = {param.get('name', ''): param for param in target_params if param.get('name')}
        
        if not source_dict and not target_dict:
            return 100.0
        if not source_dict or not target_dict:
            return 0.0
        
        # 1. å‚æ•°åç§°åŒ¹é…åº¦è®¡ç®—
        source_names = set(source_dict.keys())
        target_names = set(target_dict.keys())
        
        common_names = source_names.intersection(target_names)  # ç›¸åŒçš„å‚æ•°å
        all_names = source_names.union(target_names)            # æ‰€æœ‰å‚æ•°å
        
        # åç§°åŒ¹é…åº¦ï¼šç›¸åŒå‚æ•°åæ•°é‡ / æ‰€æœ‰å‚æ•°åæ•°é‡
        name_match_ratio = len(common_names) / len(all_names) if all_names else 1.0
        
        # 2. å‚æ•°å€¼ç›¸ä¼¼åº¦è®¡ç®—
        value_similarities = []
        
        for param_name in common_names:
            source_param = source_dict[param_name]
            target_param = target_dict[param_name]
            
            # æ¯”è¾ƒå‚æ•°å€¼
            value_similarity = self._compare_parameter_values(source_param, target_param)
            value_similarities.append(value_similarity)
        
        # å¹³å‡å‚æ•°å€¼ç›¸ä¼¼åº¦
        avg_value_similarity = sum(value_similarities) / len(value_similarities) if value_similarities else 0.0
        
        # 3. ç»¼åˆè¯„åˆ†
        # å¦‚æœæ²¡æœ‰ç›¸åŒçš„å‚æ•°åï¼Œåˆ™åªåŸºäºåç§°åŒ¹é…åº¦
        if not common_names:
            return name_match_ratio * 100.0
        
        # åç§°åŒ¹é…åº¦40% + å‚æ•°å€¼ç›¸ä¼¼åº¦60%
        final_score = (name_match_ratio * 0.4 + avg_value_similarity * 0.6) * 100.0
        
        return final_score
    
    def _compare_parameter_values(self, source_param: Dict, target_param: Dict) -> float:
        """
        æ¯”è¾ƒä¸¤ä¸ªå‚æ•°çš„å€¼ç›¸ä¼¼åº¦
        
        æ”¯æŒå¤šç§å‚æ•°ç±»å‹ï¼šæ•°å€¼ã€å­—ç¬¦ä¸²ã€å¸ƒå°”å€¼ç­‰
        """
        source_value = source_param.get('value')
        target_value = target_param.get('value')
        
        # å¦‚æœå€¼éƒ½ä¸ºç©ºï¼Œè®¤ä¸ºå®Œå…¨ç›¸åŒ
        if source_value is None and target_value is None:
            return 1.0
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªä¸ºç©ºï¼Œç›¸ä¼¼åº¦ä¸º0
        if source_value is None or target_value is None:
            return 0.0
        
        # å°è¯•æ•°å€¼æ¯”è¾ƒ
        try:
            source_num = float(source_value)
            target_num = float(target_value)
            
            # å®Œå…¨ç›¸åŒ
            if source_num == target_num:
                return 1.0
            
            # æ•°å€¼ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºç›¸å¯¹å·®å¼‚ï¼‰
            if source_num == 0 and target_num == 0:
                return 1.0
            elif source_num == 0 or target_num == 0:
                # ä¸€ä¸ªä¸º0ï¼Œå¦ä¸€ä¸ªä¸ä¸º0ï¼Œç›¸ä¼¼åº¦è¾ƒä½
                return 0.1
            else:
                # è®¡ç®—ç›¸å¯¹å·®å¼‚
                relative_diff = abs(source_num - target_num) / max(abs(source_num), abs(target_num))
                # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ (å·®å¼‚è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜)
                similarity = max(0.0, 1.0 - relative_diff)
                return similarity
                
        except (ValueError, TypeError):
            # éæ•°å€¼ç±»å‹ï¼Œè¿›è¡Œå­—ç¬¦ä¸²æ¯”è¾ƒ
            source_str = str(source_value).lower().strip()
            target_str = str(target_value).lower().strip()
            
            # å®Œå…¨ç›¸åŒ
            if source_str == target_str:
                return 1.0
            
            # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨åºåˆ—åŒ¹é…ï¼‰
            from difflib import SequenceMatcher
            similarity = SequenceMatcher(None, source_str, target_str).ratio()
            return similarity
    
    def _match_keywords(self, source_keywords: List[str], target_keywords: List[str]) -> float:
        """
        åŒ¹é…å…³é”®è¯ - æ”¹è¿›ç‰ˆæœ¬
        
        åŒæ—¶è€ƒè™‘ï¼š
        1. å…³é”®è¯ç²¾ç¡®åŒ¹é…
        2. å…³é”®è¯éƒ¨åˆ†åŒ¹é…
        3. å®Œæ•´åç§°çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
        """
        if not source_keywords and not target_keywords:
            return 100.0
        if not source_keywords or not target_keywords:
            return 0.0
        
        source_set = set(keyword.lower() for keyword in source_keywords if keyword)
        target_set = set(keyword.lower() for keyword in target_keywords if keyword)
        
        if not source_set or not target_set:
            return 0.0
        
        # 1. ç²¾ç¡®åŒ¹é…åˆ†æ•°
        common_keywords = source_set.intersection(target_set)
        max_keywords = max(len(source_set), len(target_set))
        exact_score = (len(common_keywords) / max_keywords) * 100.0 if max_keywords > 0 else 0.0
        
        # 2. éƒ¨åˆ†åŒ¹é…åˆ†æ•°ï¼ˆç”¨äºå…³é”®è¯é—´çš„æ¨¡ç³ŠåŒ¹é…ï¼‰
        partial_score = 0.0
        if exact_score < 50.0:
            partial_matches = 0
            total_comparisons = 0
            
            for source_kw in source_set:
                for target_kw in target_set:
                    # è¿‡æ»¤æ‰å¤ªçŸ­çš„å…³é”®è¯ï¼ˆé¿å…å•å­—ç¬¦è¯¯åŒ¹é…ï¼‰
                    if len(source_kw) <= 1 or len(target_kw) <= 1:
                        continue
                    
                    total_comparisons += 1
                    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»æˆ–ç›¸ä¼¼åº¦ï¼‰
                    if (source_kw in target_kw or target_kw in source_kw):
                        partial_matches += 1
                    elif SequenceMatcher(None, source_kw, target_kw).ratio() > 0.7:
                        partial_matches += 0.5  # ç›¸ä¼¼åº¦åŒ¹é…æƒé‡è¾ƒä½
            
            if total_comparisons > 0:
                partial_score = (partial_matches / total_comparisons) * 60.0  # éƒ¨åˆ†åŒ¹é…æœ€é«˜60åˆ†
        
        # 3. å®Œæ•´åç§°ç›¸ä¼¼åº¦ï¼ˆé˜²æ­¢è¿‡åº¦ä¾èµ–å…³é”®è¯ï¼‰
        # å°†æ‰€æœ‰å…³é”®è¯è¿æ¥æˆå®Œæ•´å­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
        source_full = "_".join(sorted(source_set))
        target_full = "_".join(sorted(target_set))
        string_similarity = SequenceMatcher(None, source_full, target_full).ratio() * 100.0
        
        # ç»¼åˆè¯„åˆ†ï¼šç²¾ç¡®åŒ¹é…50% + éƒ¨åˆ†åŒ¹é…30% + å­—ç¬¦ä¸²ç›¸ä¼¼åº¦20%
        if exact_score >= 50.0:
            # é«˜ç²¾ç¡®åŒ¹é…æ—¶ï¼Œä¸»è¦çœ‹ç²¾ç¡®åº¦
            final_score = exact_score * 0.7 + string_similarity * 0.3
        else:
            # ä½ç²¾ç¡®åŒ¹é…æ—¶ï¼Œç»¼åˆå„é¡¹æŒ‡æ ‡
            final_score = exact_score * 0.5 + partial_score * 0.3 + string_similarity * 0.2
        
        return max(0.0, min(100.0, final_score))
    
    def _match_sampler_paths(self, source_samplers: List[Dict], target_samplers: List[Dict]) -> float:
        """åŒ¹é…é‡‡æ ·å™¨è·¯å¾„"""
        if not source_samplers and not target_samplers:
            return 100.0
        if not source_samplers or not target_samplers:
            return 0.0   # è¿”å›0åˆ†ï¼Œç”±é›¶åˆ†ä¿æŠ¤æœºåˆ¶å¤„ç†
        
        source_paths = [sampler.get('path', '') for sampler in source_samplers]
        target_paths = [sampler.get('path', '') for sampler in target_samplers]
        
        # è®¡ç®—è·¯å¾„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarities = []
        for source_path in source_paths:
            for target_path in target_paths:
                if source_path and target_path:
                    sim = SequenceMatcher(None, source_path.lower(), target_path.lower()).ratio()
                    similarities.append(sim)
        
        if not similarities:
            return 0.0
        
        # è¿”å›å¹³å‡ç›¸ä¼¼åº¦
        return (sum(similarities) / len(similarities)) * 100.0
    
    def _extract_material_keywords(self, material_name: str) -> List[str]:
        """
        ä»æè´¨åç§°æå–å…³é”®è¯ - ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”
        
        ä¾‹å¦‚: AEG301_221_C[c2030]_BD_Fabric
        å…³é”®è¯: ['AEG301', '221', 'C[c2030]', 'BD', 'Fabric']
        """
        if not material_name:
            return []
        
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        name = material_name
        for ext in ['.matbin', '.xml', '.matxml', '.mtd']:
            if name.lower().endswith(ext):
                name = name[:-len(ext)]
        
        # æŒ‰ä¸‹åˆ’çº¿åˆ†éš”
        keywords = [kw.strip() for kw in name.split('_') if kw.strip()]
        
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„å…³é”®è¯ï¼ˆ1-2ä¸ªå­—ç¬¦çš„å¯èƒ½æ˜¯æ— æ„ä¹‰çš„ï¼‰
        keywords = [kw for kw in keywords if len(kw) >= 2]
        
        return keywords
    
    def _match_material_keywords(self, source_keywords: List[str], target_keywords: List[str]) -> float:
        """
        åŒ¹é…æè´¨å…³é”®è¯ - æŒ‰ç”¨æˆ·éœ€æ±‚çš„ç®—æ³•
        
        è®¡ç®—: å·²åŒ¹é…çš„å…³é”®è¯æ•°é‡ / æºæè´¨å…³é”®è¯æ•°é‡
        ä¾‹å¦‚: æºæè´¨æœ‰5ä¸ªå…³é”®è¯ï¼Œç›®æ ‡åŒ…å«å…¶ä¸­1ä¸ª(BD)ï¼Œåˆ™ç›¸ä¼¼åº¦ä¸º 1/5 = 20%
        """
        if not source_keywords:
            return 100.0  # æºæè´¨æ²¡æœ‰å…³é”®è¯ï¼Œè®¤ä¸ºå®Œå…¨åŒ¹é…
        if not target_keywords:
            return 0.0  # ç›®æ ‡æ²¡æœ‰å…³é”®è¯ï¼Œæ— æ³•åŒ¹é…
        
        # è½¬ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        source_set = set(kw.lower() for kw in source_keywords if kw)
        target_set = set(kw.lower() for kw in target_keywords if kw)
        
        if not source_set:
            return 100.0
        
        # è®¡ç®—æºå…³é”®è¯ä¸­æœ‰å¤šå°‘ä¸ªåœ¨ç›®æ ‡ä¸­åŒ¹é…
        matched_count = 0
        for src_kw in source_set:
            for tgt_kw in target_set:
                # ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«å…³ç³»
                if src_kw == tgt_kw or src_kw in tgt_kw or tgt_kw in src_kw:
                    matched_count += 1
                    break  # æ¯ä¸ªæºå…³é”®è¯åªåŒ¹é…ä¸€æ¬¡
        
        # ç›¸ä¼¼åº¦ = åŒ¹é…æ•° / æºå…³é”®è¯æ€»æ•°
        similarity = (matched_count / len(source_set)) * 100.0
        return similarity

    def _extract_keywords(self, text: str, pattern_type: str) -> List[str]:
        """æå–å…³é”®è¯"""
        if not text or pattern_type not in self.keyword_patterns:
            return []
        
        keywords = []
        patterns = self.keyword_patterns[pattern_type]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        return list(set(keyword for keyword in keywords if keyword.strip()))
    
    def _calculate_weights(self, priority_order: List[str]) -> Dict[str, float]:
        """æ ¹æ®ä¼˜å…ˆçº§é¡ºåºè®¡ç®—æƒé‡"""
        # å¦‚æœæ²¡æœ‰æä¾›ä¼˜å…ˆçº§é¡ºåºï¼Œä½¿ç”¨é»˜è®¤æƒé‡
        if not priority_order:
            return self.default_weights.copy()
        
        weights = {}
        
        # é¦–å…ˆä¸ºæ‰€æœ‰ç‰¹å¾åˆ†é…åŸºç¡€æƒé‡
        for feature in self.default_weights:
            weights[feature] = self.default_weights[feature]
        
        # ç„¶åæ ¹æ®ä¼˜å…ˆçº§é¡ºåºè°ƒæ•´æƒé‡
        for i, feature in enumerate(priority_order):
            if feature in self.default_weights:
                # ä¼˜å…ˆçº§è¶Šé«˜ï¼Œæƒé‡å¢å¼ºè¶Šå¤š
                priority_boost = (len(priority_order) - i) * 0.1
                weights[feature] = min(1.0, weights[feature] + priority_boost)
        
        # é‡æ–°è§„èŒƒåŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œä¸º1.0
        total_weight = sum(weights.values())
        if total_weight > 0:
            for feature in weights:
                weights[feature] = weights[feature] / total_weight
        
        return weights
    
    def _calculate_weights_with_groups(self, priority_groups: List[List[str]]) -> Dict[str, float]:
        """æ ¹æ®ä¼˜å…ˆçº§åˆ†ç»„è®¡ç®—æƒé‡ï¼ˆæ”¯æŒåŒçº§ä¼˜å…ˆçº§ï¼‰"""
        if not priority_groups:
            return self.default_weights.copy()
        
        weights = {}
        
        # é¦–å…ˆä¸ºæ‰€æœ‰ç‰¹å¾åˆ†é…åŸºç¡€æƒé‡
        for feature in self.default_weights:
            weights[feature] = self.default_weights[feature] * 0.5  # é™ä½åŸºç¡€æƒé‡
        
        # æŒ‰ç»„åˆ†é…æƒé‡
        total_groups = len(priority_groups)
        for group_index, group in enumerate(priority_groups):
            # ç»„çš„æƒé‡ï¼šä¼˜å…ˆçº§è¶Šé«˜æƒé‡è¶Šå¤§
            group_weight = (total_groups - group_index) * 0.3
            # ç»„å†…æƒé‡å‡åˆ†
            individual_weight = group_weight / len(group) if group else 0
            
            for feature in group:
                if feature in weights:
                    weights[feature] += individual_weight
        
        # é‡æ–°è§„èŒƒåŒ–æƒé‡ï¼Œç¡®ä¿æ€»å’Œä¸º1.0
        total_weight = sum(weights.values())
        if total_weight > 0:
            for feature in weights:
                weights[feature] = weights[feature] / total_weight
        
        return weights
    
    def _get_library_name(self, library_id: int) -> str:
        """è·å–åº“åç§°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if library_id in self._library_cache:
            return self._library_cache[library_id]
        
        try:
            libraries = self.database_manager.get_libraries()
            for lib in libraries:
                if lib['id'] == library_id:
                    name = lib['name']
                    self._library_cache[library_id] = name
                    return name
            name = f"Library {library_id}"
            self._library_cache[library_id] = name
            return name
        except:
            name = f"Library {library_id}"
            self._library_cache[library_id] = name
            return name
    
    def _quick_prefilter(self, source_details: Dict, target_material: Dict, threshold: float) -> bool:
        """
        å¿«é€Ÿé¢„ç­›é€‰ - åŸºäºå…³é”®ç‰¹å¾è¿›è¡Œå¿«é€Ÿç­›é€‰ï¼šåç§°ç›¸ä¼¼åº¦ã€ç€è‰²å™¨ã€é‡‡æ ·å™¨ç±»å‹
        åªæœ‰é€šè¿‡é¢„ç­›é€‰çš„æè´¨æ‰ä¼šè¿›è¡Œè¯¦ç»†çš„ç›¸ä¼¼åº¦è®¡ç®—
        """
        try:
            # 1. æè´¨åç§°å¿«é€Ÿç›¸ä¼¼åº¦æ£€æŸ¥
            source_name = source_details.get('filename', '').lower()
            target_name = target_material.get('filename', target_material.get('file_name', '')).lower()
            
            if source_name and target_name:
                # ä½¿ç”¨å¿«é€Ÿå­—ç¬¦ä¸²åŒ¹é…ç®—æ³•
                name_similarity = SequenceMatcher(None, source_name, target_name).ratio()
                # å¦‚æœåç§°ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œç›´æ¥é€šè¿‡
                if name_similarity > 0.7:
                    return True
                # å¦‚æœåç§°ç›¸ä¼¼åº¦æä½ï¼Œå¯èƒ½éœ€è¦å…¶ä»–ç‰¹å¾è¡¥å¿
                if name_similarity < 0.1:
                    # éœ€è¦æ›´å¼ºçš„å…¶ä»–ç‰¹å¾åŒ¹é…æ‰èƒ½é€šè¿‡
                    pass
            
            # 2. ç€è‰²å™¨è·¯å¾„å¿«é€ŸåŒ¹é…
            source_shader = source_details.get('shader_path', '').lower()
            target_shader = target_material.get('shader_path', '').lower()
            shader_match_score = 0.0
            
            if source_shader and target_shader:
                # æå–ç€è‰²å™¨å…³é”®è¯è¿›è¡Œå¿«é€Ÿæ¯”è¾ƒ
                source_shader_keywords = set(source_shader.split('/'))
                target_shader_keywords = set(target_shader.split('/'))
                
                # è®¡ç®—å…³é”®è¯é‡åˆåº¦
                common_keywords = source_shader_keywords.intersection(target_shader_keywords)
                max_keywords = max(len(source_shader_keywords), len(target_shader_keywords))
                
                if max_keywords > 0:
                    shader_match_score = len(common_keywords) / max_keywords
                    
                # å¦‚æœç€è‰²å™¨å®Œå…¨ä¸åŒ¹é…ï¼Œå¾ˆéš¾é€šè¿‡é¢„ç­›é€‰
                if shader_match_score < 0.2 and len(common_keywords) == 0:
                    return False
            
            # 3. é‡‡æ ·å™¨ç±»å‹å¿«é€Ÿæ£€æŸ¥ï¼ˆåªæ£€æŸ¥åŸºç¡€ä¿¡æ¯ï¼Œä¸è·å–å®Œæ•´é‡‡æ ·å™¨æ•°æ®ï¼‰
            source_samplers = source_details.get('samplers', [])
            # å¯¹äºç›®æ ‡æè´¨ï¼Œæˆ‘ä»¬åªèƒ½åšåŸºæœ¬æ£€æŸ¥ï¼Œå› ä¸ºè¿˜æ²¡æœ‰è·å–è¯¦ç»†ä¿¡æ¯
            
            # é‡‡æ ·å™¨æ•°é‡æ£€æŸ¥
            source_sampler_count = len(source_samplers)
            # å¦‚æœæºæè´¨æœ‰å¾ˆå¤šé‡‡æ ·å™¨ï¼Œç›®æ ‡æè´¨åº”è¯¥ä¹Ÿæœ‰ä¸€äº›é‡‡æ ·å™¨çš„å¯èƒ½æ€§
            # ä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•è·å–ç›®æ ‡æè´¨çš„è¯¦ç»†é‡‡æ ·å™¨ä¿¡æ¯ï¼Œæ‰€ä»¥æš‚æ—¶è·³è¿‡
            
            # 4. ç»¼åˆè¯„ä¼°
            # è®¡ç®—ä¸€ä¸ªå¿«é€Ÿçš„é¢„ç­›é€‰åˆ†æ•°
            prefilter_score = 0.0
            
            # åç§°æƒé‡ï¼š40%
            if source_name and target_name:
                name_similarity = SequenceMatcher(None, source_name, target_name).ratio()
                prefilter_score += name_similarity * 0.4
            
            # ç€è‰²å™¨æƒé‡ï¼š60%
            prefilter_score += shader_match_score * 0.6
            
            # é¢„ç­›é€‰é˜ˆå€¼ï¼šåªæœ‰è¾¾åˆ°ä¸€å®šåˆ†æ•°çš„æè´¨æ‰èƒ½é€šè¿‡
            # è¿™ä¸ªé˜ˆå€¼æ¯”æœ€ç»ˆé˜ˆå€¼è¦ä½å¾ˆå¤šï¼Œä½†è¶³ä»¥è¿‡æ»¤æ‰å¤§éƒ¨åˆ†ä¸ç›¸å…³çš„æè´¨
            prefilter_threshold = max(0.15, threshold * 0.2)  # é¢„ç­›é€‰é˜ˆå€¼ä¸ºæœ€ç»ˆé˜ˆå€¼çš„20%ï¼Œæœ€ä½15%
            
            return prefilter_score >= prefilter_threshold
            
        except Exception as e:
            # å¦‚æœé¢„ç­›é€‰å‡ºé”™ï¼Œä¿å®ˆåœ°å…è®¸è¿›å…¥è¯¦ç»†è®¡ç®—
            print(f"é¢„ç­›é€‰è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return True
    
    def get_parameter_comparison_details(self, source_params: List[Dict], target_params: List[Dict]) -> Dict:
        """
        è·å–å‚æ•°æ¯”è¾ƒçš„è¯¦ç»†ä¿¡æ¯
        
        è¿”å›åŒ…å«ä»¥ä¸‹ä¿¡æ¯çš„å­—å…¸ï¼š
        - common_count: ç›¸åŒå‚æ•°åç§°çš„æ•°é‡
        - source_only_count: ä»…æºæè´¨æœ‰çš„å‚æ•°æ•°é‡
        - target_only_count: ä»…ç›®æ ‡æè´¨æœ‰çš„å‚æ•°æ•°é‡ 
        - common_params: ç›¸åŒå‚æ•°åç§°çš„åˆ—è¡¨
        - value_match_details: ç›¸åŒå‚æ•°åç§°çš„å€¼åŒ¹é…è¯¦æƒ…
        """
        if not source_params and not target_params:
            return {
                'common_count': 0,
                'source_only_count': 0,
                'target_only_count': 0,
                'common_params': [],
                'value_match_details': []
            }
        
        # å°†å‚æ•°è½¬æ¢ä¸ºå­—å…¸
        source_dict = {param.get('name', ''): param for param in source_params if param.get('name')}
        target_dict = {param.get('name', ''): param for param in target_params if param.get('name')}
        
        source_names = set(source_dict.keys())
        target_names = set(target_dict.keys())
        
        common_names = source_names.intersection(target_names)
        source_only_names = source_names - target_names
        target_only_names = target_names - source_names
        
        # è®¡ç®—ç›¸åŒå‚æ•°çš„å€¼åŒ¹é…è¯¦æƒ…
        value_match_details = []
        for param_name in common_names:
            source_param = source_dict[param_name]
            target_param = target_dict[param_name]
            value_similarity = self._compare_parameter_values(source_param, target_param)
            
            value_match_details.append({
                'name': param_name,
                'similarity': value_similarity,
                'source_value': source_param.get('value'),
                'target_value': target_param.get('value')
            })
        
        return {
            'common_count': len(common_names),
            'source_only_count': len(source_only_names),
            'target_only_count': len(target_only_names),
            'common_params': list(common_names),
            'source_only_params': list(source_only_names),
            'target_only_params': list(target_only_names),
            'value_match_details': value_match_details
        }

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self._library_cache.clear()
        self._material_details_cache.clear()